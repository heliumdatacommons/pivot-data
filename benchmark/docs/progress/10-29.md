10/29/18
========

- [x] Use containerized NFS Ganesha for scalability
    - Used [janeczku/nfs-ganesha](https://github.com/janeczku/docker-nfs-ganesha) 
- [x] Create the playbook for Ceph RBD + NFS Ganesha
- [x] Run the single-host, single-client experiment and compare the results to the existing ones
    - Ceph RBD + NFS Ganesha improves the read performance as compared to the NFSv4 counterpart. 
    - Accessing GlusterFS using the native client is slower than using NFS Ganesha by an order of 
      magnitude.
    - The read performance of GlusterFS + NFS Ganesha significantly drops when the file size 
      increases over 200MB.
    - In overall, the throughput of random read decreases as file size increases.
    - CephFS is not good at non-sequential data access (*e.g.*, record rewrite, stride read, 
      random read/write).
- [x] Write an experiment runner
- [x] Run multiple-host, single-client experiments
    - 20 iterations with the experiment runner
