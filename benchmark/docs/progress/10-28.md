10/28/18
========

- [x] Repeat single-host, single-client tests against all the filesystems
   - GlusterFS (w/ distributed replication volume) is very slow in any kind of write operation, 
     backward read and stride read, but performs outstandingly in re-read. *The intuition is that 
     GlusterFS is good at server-end caching and suitable for sequential read-heavy workloads.*
   - GlusterFS + NFS Ganesha consistently achieves low throughput for the initial read of large 
     files (>200MB), and the reason behind is unclear. 
   - NFSv4-based solutions lead in write performance. Surprisingly, Ceph RBD + NFSv4 achieves 
     comparable write performance as compared to vanilla NFS. 
   - NFSv4 seems the bottleneck for read operations since both vanilla NFSv4 and the one over Ceph 
     RBD perform badly in most read operations. In contrast, NFS Ganesha improves the read 
     performance over NFSv4. *It is intuitive to try Ceph RBD + NFS Ganesha to see if it can be a 
     more balanced solution.*  
   - For now, CephFS seems the most balanced solution.  